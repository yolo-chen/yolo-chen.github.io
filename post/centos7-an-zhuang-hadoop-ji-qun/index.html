<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" >

<title>CentOs7安装Hadoop集群 | 一笑</title>

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="shortcut icon" href="https://yolo-chen.github.io/favicon.ico?v=1688307031787">
<link rel="stylesheet" href="https://yolo-chen.github.io/styles/main.css">



<link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" />
<script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>



    <meta name="description" content="1. 安装3台centos7服务器
1.1 配置名字hadoop01\hadoop02\hadoop03
hostnamectl set-hostname hadoop01
hostnamectl set-hostname hadoop02..." />
    <meta name="keywords" content="安装过程" />
  </head>
  <body>
    <div id="app" class="main">

      <div class="sidebar" :class="{ 'full-height': menuVisible }">
  <div class="top-container" data-aos="fade-right">
    <div class="top-header-container">
      <a class="site-title-container" href="https://yolo-chen.github.io">
        <img src="https://yolo-chen.github.io/images/avatar.png?v=1688307031787" class="site-logo">
        <h1 class="site-title">一笑</h1>
      </a>
      <div class="menu-btn" @click="menuVisible = !menuVisible">
        <div class="line"></div>
      </div>
    </div>
    <div>
      
        
          <a href="/" class="site-nav">
            首页
          </a>
        
      
        
          <a href="/archives" class="site-nav">
            归档
          </a>
        
      
        
          <a href="/tags" class="site-nav">
            标签
          </a>
        
      
    </div>
  </div>
  <div class="bottom-container" data-aos="flip-up" data-aos-offset="0">
    <div class="social-container">
      
        
      
        
      
        
      
        
      
        
      
    </div>
    <div class="site-description">
      温故而知新
    </div>
    <div class="site-footer">
      powerby <a href="https://github.com/yolo-chen">yolo-chen</a> | <a class="rss" href="https://yolo-chen.github.io/atom.xml" target="_blank">RSS</a>
    </div>
  </div>
</div>


      <div class="main-container">
        <div class="content-container" data-aos="fade-up">
          <div class="post-detail">
            <h2 class="post-title">CentOs7安装Hadoop集群</h2>
            <div class="post-date">2022-07-28</div>
            
            <div class="post-content" v-pre>
              <h2 id="1-安装3台centos7服务器">1. 安装3台centos7服务器</h2>
<h3 id="11-配置名字hadoop01hadoop02hadoop03">1.1 配置名字hadoop01\hadoop02\hadoop03</h3>
<p><code>hostnamectl set-hostname hadoop01</code><br>
<code>hostnamectl set-hostname hadoop02</code><br>
<code>hostnamectl set-hostname hadoop03</code></p>
<h3 id="12-修改hosts文件">1.2 修改hosts文件</h3>
<p><code>vi /etc/hosts</code><br>
文件末尾添加以下内容：</p>
<pre><code class="language-shell">hadoop01的ip地址 hadoop01
hadoop02的ip地址 hadoop02
hadoop03的ip地址 hadoop03
如
192.168.0.11    hadoop01
</code></pre>
<h3 id="13-关闭防火墙">1.3 关闭防火墙</h3>
<pre><code class="language-shell">systemctl stop firewalld
</code></pre>
<pre><code class="language-shell">systemctl disable firewalld
</code></pre>
<h2 id="2-xshell点击工具选择发送键输入到所有会话">2. xshell点击工具，选择发送键输入到所有会话</h2>
<h3 id="21-所有窗口状态改成no即同时输入命令到单个窗口">2.1 <strong>所有窗口状态改成NO(即同时输入命令到单个窗口)</strong></h3>
<h2 id="3-hadoop01输入以下命令">3. hadoop01输入以下命令</h2>
<h3 id="31做ssh-公私钥-无秘中途直接回车">3.1做ssh 公私钥 无秘；中途直接回车</h3>
<pre><code class="language-shell">ssh-keygen -t rsa -P ''
</code></pre>
<h3 id="32-copy公钥到hadoop02hadoop03输入yes再输入密码">3.2 copy公钥到hadoop02，hadoop03；输入yes，再输入密码</h3>
<pre><code class="language-shell">ssh-copy-id hadoop01
</code></pre>
<pre><code class="language-shell">ssh-copy-id hadoop02
</code></pre>
<pre><code class="language-shell">ssh-copy-id hadoop03
</code></pre>
<h2 id="4-测试以上操作是否成功">4. 测试以上操作是否成功</h2>
<h3 id="41-hadoop02hadoop03分别输入以下命令">4.1 hadoop02，hadoop03分别输入以下命令</h3>
<pre><code class="language-shell">cd .ssh/
ls
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://cdn.nlark.com/yuque/0/2023/png/12885594/1677567239412-a5780d9b-3d06-4378-910b-1f00fe996411.png#averageHue=%23dde1da&amp;clientId=u79462fa3-f1a0-4&amp;from=paste&amp;id=u3012b81e&amp;originHeight=161&amp;originWidth=815&amp;originalType=url&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=95085&amp;status=done&amp;style=none&amp;taskId=uaca5d6c9-cca3-467c-8b4e-ef807ce60dc&amp;title=" alt="image.png" loading="lazy"></figure>
<h3 id="42-hadoop01输入以下命令">4.2 hadoop01输入以下命令</h3>
<pre><code class="language-shell">ssh hadoop02
ssh hadoop03
exit
</code></pre>
<figure data-type="image" tabindex="2"><img src="https://cdn.nlark.com/yuque/0/2023/png/12885594/1677567288626-3942820a-dab3-4818-913c-1e255b5e9349.png#averageHue=%23d0dacf&amp;clientId=u79462fa3-f1a0-4&amp;from=paste&amp;id=u9ec3bd59&amp;originHeight=206&amp;originWidth=454&amp;originalType=url&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=115338&amp;status=done&amp;style=none&amp;taskId=ua9868665-5e58-491c-ae77-39c17216206&amp;title=" alt="image.png" loading="lazy"></figure>
<h2 id="5-第2步的基础hadoop02和hadoop03窗口状态改成off命令输入到02-和03">5. 第2步的基础，hadoop02和hadoop03窗口状态改成OFF（命令输入到02 和03）</h2>
<h3 id="51-输入以下命令和第3步一样">5.1 输入以下命令，和第3步一样</h3>
<pre><code class="language-shell">ssh-keygen -t rsa -P ''
</code></pre>
<h3 id="52-copy公钥到hadoop02hadoop03输入yes再输入密码">5.2 copy公钥到hadoop02，hadoop03；输入yes，再输入密码</h3>
<pre><code class="language-shell">ssh-copy-id hadoop01
</code></pre>
<pre><code class="language-shell">ssh-copy-id hadoop02
</code></pre>
<pre><code class="language-shell">ssh-copy-id hadoop03
</code></pre>
<h3 id="53-以上操作都完成后hadoop01hadoop02和hadoop03的窗口状态都改成off任意一个窗口按下ctrll">5.3 以上操作都完成后hadoop01，hadoop02和hadoop03的窗口状态都改成OFF，任意一个窗口按下ctrl+l</h3>
<figure data-type="image" tabindex="3"><img src="https://cdn.nlark.com/yuque/0/2023/png/12885594/1677567655190-a0cd71c6-2ecf-4b86-8790-0027157a61f1.png#averageHue=%235c5b5b&amp;clientId=u79462fa3-f1a0-4&amp;from=paste&amp;id=u05f647bd&amp;originHeight=789&amp;originWidth=1115&amp;originalType=url&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=82039&amp;status=done&amp;style=none&amp;taskId=ueeb51e5a-d64d-4425-bbb2-cbfa7fa09fb&amp;title=" alt="image.png" loading="lazy"></figure>
<h2 id="6-安装chrony">6. 安装chrony</h2>
<pre><code class="language-shell">yum -y install chrony
</code></pre>
<h2 id="7-安装wget">7. 安装wget</h2>
<pre><code class="language-shell">yum install -y gcc vim wget
</code></pre>
<h2 id="8-配置chrony">8. 配置chrony</h2>
<pre><code class="language-shell">vim /etc/chrony.conf
</code></pre>
<h3 id="81-文件添加如下内容注释掉server-0centospoolntporg-iburst">8.1 文件添加如下内容，注释掉server 0.centos.pool.ntp.org iburst</h3>
<figure data-type="image" tabindex="4"><img src="https://cdn.nlark.com/yuque/0/2023/png/12885594/1677567770667-e4724a77-6f56-4297-8488-28e7066a5af6.png#averageHue=%23f5efc3&amp;clientId=u79462fa3-f1a0-4&amp;from=paste&amp;id=ue053dfd2&amp;originHeight=522&amp;originWidth=1033&amp;originalType=url&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=410195&amp;status=done&amp;style=none&amp;taskId=u89831cc7-70e6-42af-8b3d-bfe782b02b3&amp;title=" alt="image.png" loading="lazy"></figure>
<pre><code class="language-shell">server ntp1.aliyun.com 
server ntp2.aliyun.com 
server ntp3.aliyun.com
</code></pre>
<h2 id="9-启动chrony">9. 启动chrony</h2>
<pre><code class="language-shell">systemctl start chronyd
</code></pre>
<h2 id="10-安装psmisc">10. 安装psmisc</h2>
<pre><code class="language-shell">yum install -y psmisc
</code></pre>
<h2 id="11-备份原始源">11. 备份原始源</h2>
<pre><code class="language-shell">mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup
</code></pre>
<h2 id="12-下载源">12. 下载源</h2>
<pre><code class="language-shell">wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo
</code></pre>
<h2 id="13-清除缓存">13. 清除缓存</h2>
<pre><code class="language-shell">yum clean all
yum makecache
</code></pre>
<h2 id="14-打开xftp将jdk安装包分别拖到三台机器的opt文件夹下然后执行以下命令安装jdk">14. 打开xftp，将jdk安装包分别拖到三台机器的opt文件夹下，然后执行以下命令，安装jdk</h2>
<pre><code class="language-shell">cd /opt
tar -zxf jdk-8u111-linux-x64.tar.gz
mkdir soft
mv jdk1.8.0_111/ soft/jdk180
</code></pre>
<h3 id="141-配置环境变量">14.1 配置环境变量</h3>
<pre><code class="language-shell">vim /etc/profile
</code></pre>
<pre><code class="language-shell">#java env
export JAVA_HOME=/opt/soft/jdk180
export PATH=$JAVA_HOME/bin:$PATH
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
</code></pre>
<pre><code class="language-shell">source /etc/profile
</code></pre>
<h2 id="15打开xftp将zookeeper安装包分别拖到三台机器的opt文件夹下然后执行以下命令安装zookeeper">15.打开xftp，将zookeeper安装包分别拖到三台机器的opt文件夹下，然后执行以下命令，安装zookeeper</h2>
<pre><code class="language-shell">tar -zxf zookeeper-3.4.5-cdh5.14.2.tar.gz
</code></pre>
<pre><code class="language-shell">mv zookeeper-3.4.5-cdh5.14.2 soft/zk345
</code></pre>
<h3 id="151-修改zoocfg文件">15.1 修改zoo.cfg文件</h3>
<pre><code class="language-shell">cd soft/zk345/conf/
cp zoo_sample.cfg zoo.cfg
vim zoo.cfg
</code></pre>
<p>修改dataDir=/opt/soft/zk345/datas：</p>
<pre><code class="language-shell">dataDir=/opt/soft/zk345/datas
</code></pre>
<p>文件末尾加上以下内容：</p>
<pre><code class="language-shell">server.1=192.168.239.137:2888:3888 
server.2=192.168.239.141:2888:3888 
server.3=192.168.239.142:2888:3888
</code></pre>
<h2 id="16-创建datas文件夹">16. 创建datas文件夹</h2>
<pre><code class="language-shell">cd /opt/soft/zk345/
mkdir datas
</code></pre>
<h2 id="17hadoop01hadoop02和hadoop03的窗口状态都改成on">17hadoop01，hadoop02和hadoop03的窗口状态都改成ON</h2>
<h3 id="171-hadoop01页面输入以下命令">17.1 hadoop01页面输入以下命令</h3>
<pre><code class="language-shell">cd datas
echo &quot;1&quot;&gt; myid
cat myid
</code></pre>
<h3 id="172-hadoop02页面输入以下命令">17.2 hadoop02页面输入以下命令</h3>
<pre><code class="language-shell">cd datas
echo &quot;2&quot;&gt; myid
cat myid
</code></pre>
<h3 id="173-hadoop03页面输入以下命令">17.3 hadoop03页面输入以下命令</h3>
<pre><code class="language-shell">cd datas
echo &quot;3&quot;&gt; myid
cat myid
</code></pre>
<h2 id="18-hadoop01hadoop02和hadoop03的窗口状态都改成off">18. hadoop01，hadoop02和hadoop03的窗口状态都改成OFF</h2>
<h3 id="181-配置zookeeper运行环境">18.1 配置zookeeper运行环境</h3>
<pre><code class="language-shell">vim /etc/profile
</code></pre>
<pre><code class="language-shell">#Zookeeper env
export ZOOKEEPER_HOME=/opt/soft/zk345
export PATH=$PATH:$ZOOKEEPER_HOME/bin
</code></pre>
<pre><code class="language-shell">source /etc/profile
</code></pre>
<h2 id="19-启动zookeeper集群">19. 启动zookeeper集群</h2>
<pre><code class="language-shell">zkServer.sh start
</code></pre>
<h2 id="20-jps命令查看必须要有进程quorumpeermain">20. jps命令查看，必须要有进程QuorumPeerMain</h2>
<pre><code class="language-shell">jps
</code></pre>
<h2 id="21-打开xftp将hadoop安装包分别拖到三台机器的opt文件夹下然后执行以下命令安装hadoop集群">21. 打开xftp，将Hadoop安装包分别拖到三台机器的opt文件夹下，然后执行以下命令，安装Hadoop集群</h2>
<pre><code class="language-shell">cd /opt
tar -zxf hadoop-2.6.0-cdh5.14.2.tar.gz
mv hadoop-2.6.0-cdh5.14.2 soft/hadoop260
cd soft/hadoop260/etc/hadoop
</code></pre>
<h3 id="211-添加对应各个文件夹">21.1 添加对应各个文件夹</h3>
<pre><code class="language-shell">mkdir -p /opt/soft/hadoop260/tmp 
mkdir -p /opt/soft/hadoop260/dfs/journalnode_data 
mkdir -p /opt/soft/hadoop260/dfs/edits 
mkdir -p /opt/soft/hadoop260/dfs/datanode_data
mkdir -p /opt/soft/hadoop260/dfs/namenode_data
</code></pre>
<h3 id="212-配置hadoop-envsh">21.2 配置hadoop-env.sh</h3>
<pre><code class="language-shell">vim hadoop-env.sh
</code></pre>
<p>修改JAVA_HOME和HADOOP_CONF_DIR的值如下：</p>
<pre><code class="language-shell">export JAVA_HOME=/opt/soft/jdk180 
export HADOOP_CONF_DIR=/opt/soft/hadoop260/etc/hadoop
</code></pre>
<h3 id="213-配置core-sitexml快捷键shiftg到文件末尾添加如下内容注意改机器名">21.3 配置core-site.xml，快捷键shift+G到文件末尾添加如下内容（注意改机器名！！！）</h3>
<pre><code class="language-shell">vim core-site.xml
</code></pre>
<pre><code class="language-properties">&lt;configuration&gt; 
&lt;!--指定hadoop集群在zookeeper上注册的节点名--&gt; 
&lt;property&gt; 
&lt;name&gt;fs.defaultFS&lt;/name&gt; 
&lt;value&gt;hdfs://hacluster&lt;/value&gt; 
&lt;/property&gt; 
&lt;!--指定hadoop运行时产生的临时文件--&gt; 
&lt;property&gt; 
&lt;name&gt;hadoop.tmp.dir&lt;/name&gt; 
&lt;value&gt;file:///opt/soft/hadoop260/tmp&lt;/value&gt; 
&lt;/property&gt; 
&lt;!--设置缓存大小 默认4KB--&gt; &lt;property&gt; 
&lt;name&gt;io.file.buffer.size&lt;/name&gt; 
&lt;value&gt;4096&lt;/value&gt; 
&lt;/property&gt; 
&lt;!--指定zookeeper的存放地址--&gt; 
&lt;property&gt; 
&lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; 
&lt;value&gt;hadoop01:2181,hadoop02:2181,hadoop03:2181&lt;/value&gt; 
&lt;/property&gt; 
&lt;!--配置允许root代理访问主机节点--&gt; 
&lt;property&gt; 
&lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;
&lt;value&gt;*&lt;/value&gt; 
&lt;/property&gt; 
&lt;!--配置该节点允许root用户所属的组--&gt; 
&lt;property&gt; 
&lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; 
&lt;value&gt;*&lt;/value&gt; 
&lt;/property&gt; 
&lt;/configuration&gt;
</code></pre>
<h3 id="214-配置hdfs-sitexml文件末尾添加如下内容注意改机器名">21.4 配置hdfs-site.xml，文件末尾添加如下内容（注意改机器名！！！）</h3>
<pre><code class="language-shell">vim hdfs-site.xml
</code></pre>
<pre><code class="language-properties">&lt;configuration&gt; 
&lt;property&gt; 
&lt;!--数据块默认大小128M--&gt; 
&lt;name&gt;dfs.block.size&lt;/name&gt; 
&lt;value&gt;134217728&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt; 
&lt;!--副本数量 不配置默认为3--&gt; 
&lt;name&gt;dfs.replication&lt;/name&gt; 
&lt;value&gt;3&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt; 
&lt;!--namenode节点数据(元数据)的存放位置--&gt; 
&lt;name&gt;dfs.name.dir&lt;/name&gt; 
&lt;value&gt;file:///opt/soft/hadoop260/dfs/namenode_data&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt; 
&lt;!--datanode节点数据(元数据)的存放位置--&gt; 
&lt;name&gt;dfs.data.dir&lt;/name&gt; 
&lt;value&gt;file:///opt/soft/hadoop260/dfs/datanode_data&lt;/value&gt; 
&lt;/property&gt;
&lt;property&gt;
&lt;!--开启hdfs的webui界面--&gt; 
&lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; 
&lt;value&gt;true&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt; 
&lt;!--datanode上负责进行文件操作的线程数--&gt; 
&lt;name&gt;dfs.datanode.max.transfer.threads&lt;/name&gt; 
&lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; 
&lt;property&gt; 
&lt;!--指定hadoop集群在zookeeper上的注册名--&gt; 
&lt;name&gt;dfs.nameservices&lt;/name&gt; 
&lt;value&gt;hacluster&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt; 
&lt;!--hacluster集群下有两个namenode分别是nn1,nn2--&gt; 
&lt;name&gt;dfs.ha.namenodes.hacluster&lt;/name&gt; 
&lt;value&gt;nn1,nn2&lt;/value&gt; 
&lt;/property&gt; 
&lt;!--nn1的rpc、servicepc和http通讯地址 --&gt; 
&lt;property&gt; 
&lt;name&gt;dfs.namenode.rpc-address.hacluster.nn1&lt;/name&gt; 
&lt;value&gt;hadoop01:9000&lt;/value&gt; 
&lt;/property&gt;
&lt;property&gt; 
&lt;name&gt;dfs.namenode.servicepc-address.hacluster.nn1&lt;/name&gt; 
&lt;value&gt;hadoop01:53310&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt; 
&lt;name&gt;dfs.namenode.http-address.hacluster.nn1&lt;/name&gt; 
&lt;value&gt;hadoop01:50070&lt;/value&gt; 
&lt;/property&gt; 
&lt;!--nn2的rpc、servicepc和http通讯地址 --&gt; 
&lt;property&gt; 
&lt;name&gt;dfs.namenode.rpc-address.hacluster.nn2&lt;/name&gt; 
&lt;value&gt;hadoop02:9000&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt; 
&lt;name&gt;dfs.namenode.servicepc-address.hacluster.nn2&lt;/name&gt; 
&lt;value&gt;hadoop02:53310&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt; 
&lt;name&gt;dfs.namenode.http-address.hacluster.nn2&lt;/name&gt; 
&lt;value&gt;hadoop02:50070&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt; 
&lt;!--指定Namenode的元数据在JournalNode上存放的位置--&gt; 
&lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; 
&lt;value&gt;qjournal://hadoop01:8485;hadoop02:8485;hadoop03:8485/hacluster&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt; 
&lt;!--指定JournalNode在本地磁盘的存储位置--&gt; 
&lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; 
&lt;value&gt;/opt/soft/hadoop260/dfs/journalnode_data&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt; 
&lt;!--指定namenode操作日志存储位置--&gt; 
&lt;name&gt;dfs.namenode.edits.dir&lt;/name&gt; 
&lt;value&gt;/opt/soft/hadoop260/dfs/edits&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt; 
&lt;!--开启namenode故障转移自动切换--&gt; 
&lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; 
&lt;value&gt;true&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt; 
&lt;!--配置失败自动切换实现方式--&gt; 
&lt;name&gt;dfs.client.failover.proxy.provider.hacluster&lt;/name&gt; 
&lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt; 
&lt;!--配置隔离机制--&gt; 
&lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; 
&lt;value&gt;sshfence&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt; 
&lt;!--配置隔离机制需要SSH免密登录--&gt; 
&lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; 
&lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;
&lt;/property&gt; 
&lt;property&gt; 
&lt;!--hdfs文件操作权限 false为不验证--&gt; 
&lt;name&gt;dfs.premissions&lt;/name&gt; 
&lt;value&gt;false&lt;/value&gt; 
&lt;/property&gt; 
&lt;/configuration&gt;
</code></pre>
<h3 id="215-配置mapred-sitexml文件末尾添加如下内容注意改机器名">21.5 配置mapred-site.xml，文件末尾添加如下内容（注意改机器名！！！）</h3>
<pre><code class="language-shell">cp mapred-site.xml.template mapred-site.xml
vim mapred-site.xml
</code></pre>
<pre><code class="language-properties">&lt;configuration&gt; 
&lt;property&gt; 
&lt;!--指定mapreduce在yarn上运行--&gt; 
&lt;name&gt;mapreduce.framework.name&lt;/name&gt; 
&lt;value&gt;yarn&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt; 
&lt;!--配置历史服务器地址--&gt; 
&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; 
&lt;value&gt;hadoop01:10020&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt; 
&lt;!--配置历史服务器webUI地址--&gt; 
&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; 
&lt;value&gt;hadoop01:19888&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt; 
&lt;!--开启uber模式--&gt; 
&lt;name&gt;mapreduce.job.ubertask.enable&lt;/name&gt; 
&lt;value&gt;true&lt;/value&gt; 
&lt;/property&gt; 
&lt;/configuration&gt;
</code></pre>
<h3 id="216-配置yarn-sitexml文件末尾添加如下内容注意改机器名">21.6 配置yarn-site.xml，文件末尾添加如下内容（注意改机器名！！！）</h3>
<pre><code class="language-shell">vim yarn-site.xml
</code></pre>
<pre><code class="language-properties">&lt;configuration&gt; 
&lt;property&gt; 
&lt;!--开启yarn高可用--&gt; 
&lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; 
&lt;value&gt;true&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt; 
&lt;!-- 指定Yarn集群在zookeeper上注册的节点名--&gt; 
&lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; 
&lt;value&gt;hayarn&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt; 
&lt;!--指定两个resourcemanager的名称--&gt; 
&lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; 
&lt;value&gt;rm1,rm2&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt; 
&lt;!--指定rm1的主机--&gt; 
&lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; 
&lt;value&gt;hadoop02&lt;/value&gt; 
&lt;/property&gt;
&lt;property&gt; 
&lt;!--指定rm2的主机--&gt; 
&lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; 
&lt;value&gt;hadoop03&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt; 
&lt;!--配置zookeeper的地址--&gt; 
&lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; 
&lt;value&gt;hadoop01:2181,hadoop02:2181,hadoop03:2181&lt;/value&gt; 
&lt;/property&gt; &lt;property&gt; 
&lt;!--开启yarn恢复机制--&gt; 
&lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; 
&lt;value&gt;true&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt; 
&lt;!--配置执行resourcemanager恢复机制实现类--&gt; 
&lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; 
&lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt; 
&lt;!--指定主resourcemanager的地址--&gt; 
&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; 
&lt;value&gt;hadoop03&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt; 
&lt;!--nodemanager获取数据的方式--&gt; 
&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; 
&lt;value&gt;mapreduce_shuffle&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt; 
&lt;!--开启日志聚集功能--&gt; 
&lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; 
&lt;value&gt;true&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt; 
&lt;!--配置日志保留7天--&gt; 
&lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; 
&lt;value&gt;604800&lt;/value&gt; 
&lt;/property&gt; 
&lt;/configuration&gt;
</code></pre>
<h2 id="22-配置slaves">22. 配置slaves</h2>
<pre><code class="language-shell">vim slaves
</code></pre>
<h3 id="221-快捷键dd删除localhost添加如下内容">22.1 快捷键dd删除localhost，添加如下内容</h3>
<pre><code class="language-shell">hadoop01
hadoop02
hadoop03
</code></pre>
<h2 id="23-配置hadoop环境变量">23. 配置hadoop环境变量</h2>
<pre><code class="language-shell">vim /etc/profile
</code></pre>
<pre><code class="language-shell">#hadoop env
export HADOOP_HOME=/opt/soft/hadoop260 
export HADOOP_MAPRED_HOME=$HADOOP_HOME 
export HADOOP_COMMON_HOME=$HADOOP_HOME 
export HADOOP_HDFS_HOME=$HADOOP_HOME 
export YARN_HOME=$HADOOP_HOME 
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native 
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin 
export HADOOP_INSTALL=$HADOOP_HOME
</code></pre>
<pre><code class="language-shell">source /etc/profile
</code></pre>
<h2 id="24-启动hadoop集群">24. 启动Hadoop集群</h2>
<h3 id="241-输入以下命令">24.1 输入以下命令</h3>
<pre><code class="language-shell">hadoop-daemon.sh start journalnode
</code></pre>
<h3 id="242-输入jps命令会发现多了一个进程journalnode">24.2 输入jps命令，会发现多了一个进程JournalNode</h3>
<pre><code class="language-shell">jps
</code></pre>
<h3 id="243-格式化namenode只在hadoop01主机上hadoop02和hadoop03的窗口状态改成on">24.3 格式化namenode(只在hadoop01主机上)（hadoop02和hadoop03的窗口状态改成ON）</h3>
<pre><code class="language-shell">hdfs namenode -format
</code></pre>
<h3 id="244-将hadoop01上的namenode的元数据复制到hadoop02相同位置">24.4 将hadoop01上的Namenode的元数据复制到hadoop02相同位置</h3>
<pre><code class="language-shell">scp -r /opt/soft/hadoop260/dfs/namenode_data/current/ root@hadoop02:/opt/soft/hadoop260/dfs/namenode_data
</code></pre>
<h3 id="245-在hadoop01上格式化故障转移控制器zkfc">24.5 在hadoop01上格式化故障转移控制器zkfc</h3>
<pre><code class="language-shell">hdfs zkfc -formatZK
</code></pre>
<h3 id="246-在hadoop01上启动dfs服务再输入jps查看进程">24.6 在hadoop01上启动dfs服务，再输入jps查看进程</h3>
<pre><code class="language-shell">start-dfs.sh
</code></pre>
<pre><code class="language-shell">jps
</code></pre>
<figure data-type="image" tabindex="5"><img src="https://cdn.nlark.com/yuque/0/2023/png/12885594/1677569056592-f3bed9c2-4c47-4eb3-b21a-9c1aa5675992.png#averageHue=%23bec2bd&amp;clientId=u79462fa3-f1a0-4&amp;from=paste&amp;id=ud874e8bc&amp;originHeight=131&amp;originWidth=251&amp;originalType=url&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=61421&amp;status=done&amp;style=none&amp;taskId=uef569f8c-1ddd-450e-a8cb-43af9376646&amp;title=" alt="image.png" loading="lazy"></figure>
<h3 id="247-在hadoop03上启动yarn服务再输入jps查看进程">24.7 在hadoop03上启动yarn服务，再输入jps查看进程</h3>
<pre><code class="language-shell">start-yarn.sh
</code></pre>
<pre><code class="language-shell">jps
</code></pre>
<figure data-type="image" tabindex="6"><img src="https://cdn.nlark.com/yuque/0/2023/png/12885594/1677569093963-30b86fe8-73d4-4b56-9e82-d26858b05a22.png#averageHue=%23ccd8cb&amp;clientId=u79462fa3-f1a0-4&amp;from=paste&amp;id=u233a6a85&amp;originHeight=143&amp;originWidth=218&amp;originalType=url&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=62798&amp;status=done&amp;style=none&amp;taskId=u2ebaba90-26f3-40a0-a6fe-faadb119e24&amp;title=" alt="image.png" loading="lazy"></figure>
<h3 id="248-在hadoop02上输入jps查看进程如下图">24.8 在hadoop02上输入jps查看进程，如下图</h3>
<figure data-type="image" tabindex="7"><img src="https://cdn.nlark.com/yuque/0/2023/png/12885594/1677569112569-c1fbf2e7-7d80-431a-8a62-5524c049a609.png#averageHue=%23eaebe6&amp;clientId=u79462fa3-f1a0-4&amp;from=paste&amp;id=u3877247f&amp;originHeight=151&amp;originWidth=243&amp;originalType=url&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=61003&amp;status=done&amp;style=none&amp;taskId=u2a6fe061-cbbc-4c53-b756-76fec6d3e1a&amp;title=" alt="image.png" loading="lazy"></figure>
<h3 id="249-在hadoop01上启动history服务器jps则会多了一个jobhistoryserver的进程">24.9 在hadoop01上启动history服务器，jps则会多了一个JobHistoryServer的进程</h3>
<pre><code class="language-shell">mr-jobhistory-daemon.sh start historyserver
</code></pre>
<pre><code class="language-shell">jps
</code></pre>
<h3 id="2410-在hadoop02上启动resourcemanager服务jps则会多了一个resourcemanager的进程">24.10 在hadoop02上启动resourcemanager服务，jps则会多了一个Resourcemanager的进程</h3>
<pre><code class="language-shell">yarn-daemon.sh start resourcemanager
</code></pre>
<pre><code class="language-shell">jps
</code></pre>
<h2 id="25-检查集群情况">25. 检查集群情况</h2>
<h3 id="251-在hadoop01上查看服务状态hdfs-haadmin-getservicestate-nn1则会对应显示activenn2则显示standby">25.1 在hadoop01上查看服务状态，hdfs haadmin -getServiceState nn1则会对应显示active，nn2则显示standby</h3>
<pre><code class="language-shell">hdfs haadmin -getServiceState nn1
</code></pre>
<pre><code class="language-shell">hdfs haadmin -getServiceState nn2
</code></pre>
<h3 id="252-在hadoop03上查看resourcemanager状态yarn-rmadmin-getservicestate-rm1则会对应显示standbyrm2则显示active">25.2 在hadoop03上查看resourcemanager状态，yarn rmadmin -getServiceState rm1则会对应显示standby，rm2则显示active</h3>
<pre><code class="language-shell">yarn rmadmin -getServiceState rm1
</code></pre>
<pre><code class="language-shell">yarn rmadmin -getServiceState rm2
</code></pre>
<h2 id="26-浏览器输入ip地址50070对比以下图片">26. 浏览器输入IP地址:50070，对比以下图片</h2>
<h3 id="261-hadoop01的ip地址注意查看是否为active">26.1 hadoop01的IP地址，注意查看是否为“active”</h3>
<figure data-type="image" tabindex="8"><img src="https://cdn.nlark.com/yuque/0/2023/png/12885594/1677569236074-d43ef328-1f59-42ce-ba4f-067dd3f477ac.png#averageHue=%23f4f3f2&amp;clientId=u79462fa3-f1a0-4&amp;from=paste&amp;id=u0245ca9c&amp;originHeight=344&amp;originWidth=745&amp;originalType=url&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=37824&amp;status=done&amp;style=none&amp;taskId=u66b1bd93-9a46-45b4-9365-224df1e4908&amp;title=" alt="image.png" loading="lazy"></figure>
<h3 id="262-hadoop02的ip地址注意查看是否为standby">26.2 hadoop02的IP地址，注意查看是否为“standby”</h3>
<figure data-type="image" tabindex="9"><img src="https://cdn.nlark.com/yuque/0/2023/png/12885594/1677569265287-a6a4bb70-4152-42b6-908c-1db64e2129f4.png#averageHue=%23f5f4f3&amp;clientId=u79462fa3-f1a0-4&amp;from=paste&amp;id=uc419cc68&amp;originHeight=376&amp;originWidth=679&amp;originalType=url&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=39837&amp;status=done&amp;style=none&amp;taskId=u333c0c52-df25-4ebc-ac53-8e7299ef12d&amp;title=" alt="image.png" loading="lazy"></figure>
<h3 id="263-最后选择上方的datanodes查看是否是三个节点如何是则高可用hadoop集群搭建成功">26.3 最后选择上方的Datanodes，查看是否是三个节点，如何是，则高可用hadoop集群搭建成功！！！</h3>
<figure data-type="image" tabindex="10"><img src="https://cdn.nlark.com/yuque/0/2023/png/12885594/1677569281817-a384227d-aa40-4458-9954-a49884036adc.png#averageHue=%23fafaf9&amp;clientId=u79462fa3-f1a0-4&amp;from=paste&amp;id=uc0952956&amp;originHeight=970&amp;originWidth=1251&amp;originalType=url&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=105728&amp;status=done&amp;style=none&amp;taskId=u2a2acde7-fe1c-4f59-a88d-f7296aa3e0a&amp;title=" alt="image.png" loading="lazy"></figure>

            </div>
            
              <div class="tag-container">
                
                  <a href="https://yolo-chen.github.io/tag/YsT8BEt1Z/" class="tag">
                    安装过程
                  </a>
                
              </div>
            
            

            

          </div>

        </div>
      </div>
    </div>

    <script src="https://unpkg.com/aos@next/dist/aos.js"></script>
<script type="application/javascript">

AOS.init();

var app = new Vue({
  el: '#app',
  data: {
    menuVisible: false,
  },
})

</script>


  <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.5.1/build/highlight.min.js"></script>
  <script>
    hljs.initHighlightingOnLoad()
  </script>





  </body>
</html>
